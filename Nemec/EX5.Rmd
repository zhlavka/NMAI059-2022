---
title: "NMAI061-22-EX5"
author: 'Matej Nemec'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Visualization of multi-dimensional data and Principal Component Analysis

## 1)Select appropriate data and try to select predictor variables (potentially with applied transformations) in order for predicted values to fit the observed data as good as possible.
We decided to go with the Animals2 dataset which only has 2 variables and we will be trying to predict brain wight based on animals body weight. 
Right away it makes intuitive sense not expect a completely linear relationship. But let us test this assumption a bit. 
```{r load}
library(robustbase)
library(sigmoid)
library(ggpubr)
library(lmtest)
data=Animals2
cor.test(data$body,data$brain)
```
As we can see when not trasforming the variables at all there seems to be no correlation. Because the data is on mamals what we would intuitively expect is that there is both a point under which the brain size doesn't really decrease (or not nearly as much) with decreasing body size and at the same time we would expect brain size increase to plateau with very large body sizes. 
When we think about which functions could model such behavior, log, tanh and sigmoid come to mind. It would also make sense to test simply taking square roots of the body size. 
```{r corr}
cor.test(log(data$body),data$brain)
cor.test(tanh(data$body),data$brain)
cor.test(sigmoid(data$body),data$brain)
best=0
best_r=0
for(i in 1:10000){
  r=cor(data$body^(1/i),data$brain)
  if(r>best_r){
    best_r=r
    best=i
  }
}
print(paste0(best, '-th root of bodyweight has the best correlation with brain weight.'))
cor.test(data$body^(1/best),data$brain)
```
We can see that best correlation is achieved with 10-th root of bodyweight. It is still not very high though.
## 2)Visualize the data including fitted values from a selected linear model.
```{r model1}
m1=lm(brain~body,data=data)
plot(data$body,data$brain)
abline(m1)
```
We can see that there is one extreme outlier in the data we should try and remove it to help our model. 
```{r model2}
data=data[data$body!=max(data$body),]
m2=lm(brain~body,data=data)
plot(data$body,data$brain)
abline(m2)
```
This seems better. Still not a great model. But sometimes best we can get is not that good. Especially with linear models.

## 3)Comment on coefficient statistical significance. Test the null hypothesis of all coeffiecients being equal to 0.
```{r modelsum}
summary(m1)
summary(m2)

```
For the first model we have not reached statistical significance, however when we remove the outlier we do. While the coefficient is significant the $R^2$ is very poor
Now we can try to fit our best transformation - $10-th$ root of bodyweight.
```{r model3}
m3=lm(brain~I(body^(1/10)),data=data)
summary(m3)
```
It is better but still not very good.
We perform anova.
```{r res}
anova(m3)
coef=summary(m3)$coefficients[2,1] 
err=summary(m3)$coefficients[2,2] 
coef + c(-1,1)*err*qt(0.975, 42)
```
We cannot reject the hypothesis that coefficient is 0. We even reaffirm this by looking at the coefficient 95% confidence interval.

## 3)Calculate residuals a based on appropriate visualizations comment on fulfilling the prerequisites of the selected model (heteroskedascity, normality, regression model shape, etc.)
We use the appropriate function to get residuals and test heteroskedacity using studentized Breusch-Pagan test.
```{r hetero}
rst=rstudent(m3)
bptest(m3)
```
We can reject the null therefore hetoroskedacity is present.
```{r heteroplot}
plot(rst~fitted(m3))
abline(0,0)
```
Maybe if we removed more outliers it wouldn't be, but that is probably not valid. (Removing the two dinosaurs would make some sense, but we already removed one and still even if we removed the other that would not fix everything.)

```{r normplot}
ggqqplot(rst)
```
There seem to be 2 outliers other than that our residuals look normal. 
Formula for our model is $Y=X^{1/10}$ which gives it this shape:
```{r shape}
curve(x^(1/10), from=1, to=50, , xlab="x", ylab="y")
```